"""
PPOå¾®è°ƒè®­ç»ƒè„šæœ¬
ä½¿ç”¨BCåˆå§‹åŒ–çš„æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒ
"""
import os
import argparse
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from environment import BomberEnv
from gymnasium.wrappers import FlattenObservation


def make_env(rank, seed=0):
    """
    åˆ›å»ºç¯å¢ƒçš„å·¥å‚å‡½æ•°
    
    Args:
        rank: ç¯å¢ƒç¼–å·
        seed: éšæœºç§å­
    """
    def _init():
        base_env = BomberEnv()
        env = FlattenObservation(base_env)
        env = Monitor(env)  # è®°å½•è®­ç»ƒç»Ÿè®¡
        env.reset(seed=seed + rank)
        return env
    return _init


def train_ppo(
    bc_model_path="./bc_models_imitation/bc_ppo_ready.zip",
    output_dir="./ppo_models",
    total_timesteps=1_000_000,
    n_envs=4,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    save_freq=50_000,
    eval_freq=25_000,
    continue_training=False,
    checkpoint_path=None,
):
    """
    PPOå¾®è°ƒè®­ç»ƒ
    
    Args:
        bc_model_path: BCåˆå§‹åŒ–çš„PPOæ¨¡å‹è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
        n_envs: å¹¶è¡Œç¯å¢ƒæ•°é‡
        learning_rate: å­¦ä¹ ç‡
        n_steps: æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
        batch_size: æ‰¹å¤§å°
        n_epochs: PPOæ›´æ–°çš„epochs
        save_freq: ä¿å­˜é¢‘ç‡
        eval_freq: è¯„ä¼°é¢‘ç‡
        continue_training: æ˜¯å¦ç»§ç»­è®­ç»ƒ
        checkpoint_path: ç»§ç»­è®­ç»ƒçš„checkpointè·¯å¾„
    """
    print("="*60)
    print("PPOå¾®è°ƒè®­ç»ƒ")
    print("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/checkpoints", exist_ok=True)
    os.makedirs(f"{output_dir}/eval", exist_ok=True)
    
    print(f"\né…ç½®:")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  æ€»æ­¥æ•°: {total_timesteps:,}")
    print(f"  å¹¶è¡Œç¯å¢ƒ: {n_envs}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    print(f"  n_steps: {n_steps}")
    print(f"  batch_size: {batch_size}")
    print(f"  n_epochs: {n_epochs}")
    
    # 1. åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    print(f"\n{'='*60}")
    print("åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    print(f"{'='*60}\n")
    
    if n_envs > 1:
        # å¤šè¿›ç¨‹å¹¶è¡Œç¯å¢ƒ
        env = SubprocVecEnv([make_env(i) for i in range(n_envs)])
        print(f"âœ… åˆ›å»ºäº† {n_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ (SubprocVecEnv)")
    else:
        # å•ç¯å¢ƒ
        env = DummyVecEnv([make_env(0)])
        print(f"âœ… åˆ›å»ºäº†å•ç¯å¢ƒ (DummyVecEnv)")
    
    # 2. åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = DummyVecEnv([make_env(100)])
    print(f"âœ… åˆ›å»ºäº†è¯„ä¼°ç¯å¢ƒ")
    
    # 3. åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹
    print(f"\n{'='*60}")
    print("åŠ è½½æ¨¡å‹...")
    print(f"{'='*60}\n")
    
    if continue_training and checkpoint_path and os.path.exists(checkpoint_path):
        # ç»§ç»­è®­ç»ƒç°æœ‰æ¨¡å‹
        print(f"ğŸ“‚ ä»checkpointç»§ç»­è®­ç»ƒ: {checkpoint_path}")
        model = PPO.load(checkpoint_path, env=env)
        print(f"âœ… CheckpointåŠ è½½æˆåŠŸ")
        
    elif os.path.exists(bc_model_path):
        # ä»BCåˆå§‹åŒ–çš„PPOå¼€å§‹
        print(f"ğŸ“‚ åŠ è½½BCåˆå§‹åŒ–çš„PPO: {bc_model_path}")
        model = PPO.load(bc_model_path, env=env)
        
        # å¯é€‰ï¼šè°ƒæ•´PPOè¶…å‚æ•°ï¼ˆBCè®­ç»ƒæ—¶å¯èƒ½ç”¨çš„æ˜¯é»˜è®¤å€¼ï¼‰
        model.learning_rate = learning_rate
        model.n_steps = n_steps
        model.batch_size = batch_size
        model.n_epochs = n_epochs
        
        print(f"âœ… BCæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   å·²è°ƒæ•´è®­ç»ƒè¶…å‚æ•°")
        
    else:
        # ä»å¤´å¼€å§‹è®­ç»ƒPPO
        print(f"âš ï¸  æœªæ‰¾åˆ°BCæ¨¡å‹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        print(f"   å°è¯•è·¯å¾„: {bc_model_path}")
        
        # ä½¿ç”¨ä¸BCä¸€è‡´çš„ç½‘ç»œç»“æ„
        policy_kwargs = dict(
            net_arch=dict(pi=[256, 128, 64], vf=[256, 128, 64])
        )
        
        model = PPO(
            policy="MlpPolicy",  # Flattened observationä½¿ç”¨MLP
            env=env,
            policy_kwargs=policy_kwargs,  # æŒ‡å®šç½‘ç»œç»“æ„
            learning_rate=learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=n_epochs,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            clip_range_vf=None,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
            use_sde=False,
            sde_sample_freq=-1,
            target_kl=None,
            tensorboard_log=f"{output_dir}/tensorboard",
            verbose=1,
        )
        print(f"âœ… æ–°PPOæ¨¡å‹åˆ›å»ºæˆåŠŸ (net_arch=[256, 128, 64])")
    
    # 4. è®¾ç½®callbacks
    print(f"\n{'='*60}")
    print("è®¾ç½®è®­ç»ƒcallbacks...")
    print(f"{'='*60}\n")
    
    callbacks = []
    
    # Checkpoint callback
    checkpoint_callback = CheckpointCallback(
        save_freq=save_freq,
        save_path=f"{output_dir}/checkpoints",
        name_prefix="ppo_checkpoint",
        save_replay_buffer=False,
        save_vecnormalize=True,
    )
    callbacks.append(checkpoint_callback)
    print(f"âœ… Checkpointæ¯ {save_freq:,} æ­¥ä¿å­˜ä¸€æ¬¡")
    
    # Eval callback
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f"{output_dir}/eval",
        log_path=f"{output_dir}/eval",
        eval_freq=eval_freq,
        n_eval_episodes=5,
        deterministic=True,
        render=False,
    )
    callbacks.append(eval_callback)
    print(f"âœ… è¯„ä¼°æ¯ {eval_freq:,} æ­¥æ‰§è¡Œä¸€æ¬¡")
    
    # 5. å¼€å§‹è®­ç»ƒ
    print(f"\n{'='*60}")
    print(f"å¼€å§‹PPOè®­ç»ƒ...")
    print(f"{'='*60}\n")
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=callbacks,
            log_interval=10,
            tb_log_name="ppo_run",
            reset_num_timesteps=not continue_training,
            progress_bar=True,
        )
        
        print(f"\n{'='*60}")
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"{'='*60}\n")
        
    except KeyboardInterrupt:
        print(f"\n{'='*60}")
        print("âš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"{'='*60}\n")
    
    # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = f"{output_dir}/ppo_finetuned.zip"
    model.save(final_model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    # 7. æ€»ç»“
    print(f"\n{'='*60}")
    print("è®­ç»ƒå®Œæˆæ€»ç»“")
    print(f"{'='*60}")
    print(f"âœ… æœ€ç»ˆæ¨¡å‹: {final_model_path}")
    print(f"âœ… æœ€ä½³æ¨¡å‹: {output_dir}/eval/best_model.zip")
    print(f"âœ… Checkpoints: {output_dir}/checkpoints/")
    print(f"âœ… TensorBoardæ—¥å¿—: {output_dir}/tensorboard/")
    print(f"\næŸ¥çœ‹è®­ç»ƒæ›²çº¿:")
    print(f"  tensorboard --logdir {output_dir}/tensorboard")
    print(f"\nä½¿ç”¨æ¨¡å‹:")
    print(f"  ä¿®æ”¹robot.pyä¸­çš„PPO_FINETUNED_PATH = '{final_model_path}'")
    print(f"{'='*60}\n")
    
    # æ¸…ç†
    env.close()
    eval_env.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="PPOå¾®è°ƒè®­ç»ƒ")
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument(
        "--bc_model", 
        type=str, 
        default="./bc_models_imitation/bc_ppo_ready.zip",
        help="BCåˆå§‹åŒ–çš„PPOæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--output_dir", 
        type=str, 
        default="./ppo_models",
        help="è¾“å‡ºç›®å½•"
    )
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument(
        "--timesteps", 
        type=int, 
        default=1_000_000,
        help="æ€»è®­ç»ƒæ­¥æ•°"
    )
    parser.add_argument(
        "--n_envs", 
        type=int, 
        default=4,
        help="å¹¶è¡Œç¯å¢ƒæ•°é‡"
    )
    parser.add_argument(
        "--lr", 
        type=float, 
        default=3e-4,
        help="å­¦ä¹ ç‡"
    )
    parser.add_argument(
        "--n_steps", 
        type=int, 
        default=2048,
        help="æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°"
    )
    parser.add_argument(
        "--batch_size", 
        type=int, 
        default=64,
        help="æ‰¹å¤§å°"
    )
    parser.add_argument(
        "--n_epochs", 
        type=int, 
        default=10,
        help="PPOæ›´æ–°epochs"
    )
    
    # Callbackå‚æ•°
    parser.add_argument(
        "--save_freq", 
        type=int, 
        default=50_000,
        help="Checkpointä¿å­˜é¢‘ç‡"
    )
    parser.add_argument(
        "--eval_freq", 
        type=int, 
        default=25_000,
        help="è¯„ä¼°é¢‘ç‡"
    )
    
    # ç»§ç»­è®­ç»ƒ
    parser.add_argument(
        "--continue_training",
        action="store_true",
        help="ç»§ç»­è®­ç»ƒç°æœ‰æ¨¡å‹"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default=None,
        help="ç»§ç»­è®­ç»ƒçš„checkpointè·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # è®­ç»ƒ
    train_ppo(
        bc_model_path=args.bc_model,
        output_dir=args.output_dir,
        total_timesteps=args.timesteps,
        n_envs=args.n_envs,
        learning_rate=args.lr,
        n_steps=args.n_steps,
        batch_size=args.batch_size,
        n_epochs=args.n_epochs,
        save_freq=args.save_freq,
        eval_freq=args.eval_freq,
        continue_training=args.continue_training,
        checkpoint_path=args.checkpoint,
    )

