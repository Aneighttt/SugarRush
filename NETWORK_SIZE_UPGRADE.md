# 网络容量升级说明

## 🔍 问题诊断

### 观察空间详解
```python
grid_view:     (14, 16, 28)  → Flatten → 6272维
  ├─ 14个通道 (墙、箱子、炸弹、玩家、道具等)
  └─ 每个通道都是完整的 16×28 地图

player_state:  (10,)  → 10维
  ├─ 位置 (x, y)
  ├─ 炸弹数量、范围
  ├─ 速度、buff状态
  └─ 生命值等

总输入维度: 6272 + 10 = 6282维
```

### 之前的网络结构 ❌
```
输入: 6282维
  ↓
[64, 64] 隐藏层  ← 严重的信息瓶颈！
  ↓
输出: 12维 (MultiDiscrete)

参数量: 约 400K
prob_true_act: 0.48 - 0.51
```

**问题**: 6282维压缩到64维 = **98.9%的信息丢失**！

---

## ✅ 升级后的网络结构

### 新网络架构
```
输入: 6282维
  ↓
[256, 128, 64] 隐藏层  ← 渐进式压缩
  ↓
输出: 12维 (MultiDiscrete)

参数量: 约 1.9M (提升4.75倍)
预期 prob_true_act: 0.60 - 0.75
```

**改进**:
- 第1层 256维: 捕获地图的高维特征
- 第2层 128维: 中间抽象表示
- 第3层 64维: 最终策略特征

---

## 📊 容量对比

| 层级 | 旧网络 | 新网络 | 提升 |
|------|--------|--------|------|
| **输入层** | 6282 | 6282 | - |
| **隐藏层1** | 64 | **256** | 4x ✓ |
| **隐藏层2** | 64 | **128** | 2x ✓ |
| **隐藏层3** | - | **64** | +1层 ✓ |
| **总参数** | ~400K | ~1.9M | 4.75x ✓ |
| **信息保留** | 1% | 4% | 4x ✓ |

---

## 🎯 预期效果提升

### 训练指标
```
旧网络 (64, 64):
  prob_true_act:  0.48 - 0.51
  direction_acc:  0.70?
  bomb_acc:       0.30?  ← 难以学习稀有动作
  speed_acc:      0.50?

新网络 (256, 128, 64):
  prob_true_act:  0.60 - 0.75  ⬆️ +20-50%
  direction_acc:  0.80+        ⬆️
  bomb_acc:       0.65+        ⬆️ 大幅提升
  speed_acc:      0.70+        ⬆️
```

### 游戏表现
- ✅ 更精确的地图理解
- ✅ 更好的时机判断（何时放炸弹）
- ✅ 更细腻的走位控制（速度档位选择）

---

## ⚙️ 已自动更新的文件

1. ✅ `train_bc_weighted.py` - 加权BC训练
2. ✅ `train_bc_imitation.py` - 原版BC训练
3. ✅ 两者网络结构统一为 `[256, 128, 64]`

---

## 🚀 立即重新训练

### 删除旧模型（可选）
```bash
# 旧模型是64维的，与新模型不兼容
rm -rf bc_models_imitation/
rm -rf bc_models_weighted/
```

### 开始训练
```bash
# 使用新网络训练（200 epochs，约3-4小时）
./TRAIN_WEIGHTED.sh
```

或者
```bash
source ~/Desktop/workspace/venv/bin/activate

python train_bc_weighted.py \
    --n_epochs 200 \
    --batch_size 128 \
    --lr 0.0003
```

---

## 📈 训练建议

### 学习率调整
- 网络更大 → 需要更多训练时间
- **建议**: 降低学习率到 `0.0002` 或 `0.0001`
- **或者**: 增加 epochs 到 300-500

### Batch Size
- 当前 128 → 如果内存足够，可以增加到 256
- 更大的 batch size 有助于大网络的稳定训练

### 监控指标
重点关注训练日志中的：
```
prob_true_act   - 总体准确率（目标 >0.65）
bomb_acc        - 炸弹准确率（目标 >0.60）
speed_acc       - 速度准确率（目标 >0.65）
```

---

## ⚠️ 注意事项

### 1. 模型不兼容
- 旧的 `[64, 64]` 模型无法加载到新的 `[256, 128, 64]` 网络
- 需要重新训练所有模型

### 2. 训练时间
- 参数量增加 → 训练时间增加约 2-3倍
- 200 epochs: 旧网络2小时 → 新网络**4-6小时**

### 3. PPO微调
- 后续PPO训练也需要使用相同的 `[256, 128, 64]` 结构
- `train_ppo.py` 需要同步更新（如果存在）

---

## 💡 为什么不用CNN？

虽然 `grid_view` 是图像数据，理论上CNN更合适，但：

❌ **CNN的问题**:
- 需要自定义 Policy 网络
- SB3 的 `ActorCriticPolicy` 默认是 MLP
- 实现复杂，PPO兼容性差

✅ **大MLP的优势**:
- 完全兼容 SB3 和 imitation 库
- 无需修改核心代码
- 参数量足够的 MLP 也能学好
- 更容易调试和维护

**结论**: 现阶段先用大MLP，如果仍然不够再考虑CNN。

---

## 🎯 下一步

1. **立即**: 使用新网络重新训练
2. **监控**: 观察 `prob_true_act` 是否超过 0.60
3. **测试**: 训练完成后在游戏中测试
4. **迭代**: 如果仍不够，考虑:
   - 进一步增大到 `[512, 256, 128]`
   - 使用 CNN 处理 grid_view
   - 收集更多专家数据

---

**现在可以开始训练了！预期 prob_true_act 将达到 0.60-0.75** 🚀

