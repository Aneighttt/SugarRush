"""
BCè®­ç»ƒ - åŠ æƒæŸå¤±ç‰ˆæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰
ä½¿ç”¨ imitation åº“ + è‡ªå®šä¹‰åŠ æƒæŸå¤±
"""
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pickle
import glob
import os
from typing import List, Dict
from imitation.algorithms import bc
from imitation.data import types
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3 import PPO
import warnings

warnings.filterwarnings("ignore", message="Converting a tensor with requires_grad=True")


def analyze_action_distribution(data_dir: str) -> Dict:
    """åˆ†æåŠ¨ä½œåˆ†å¸ƒï¼Œè®¡ç®—ç±»åˆ«æƒé‡"""
    print(f"\n{'='*60}")
    print("åˆ†æåŠ¨ä½œåˆ†å¸ƒ...")
    print(f"{'='*60}\n")
    
    total_direction = np.zeros(5, dtype=int)
    total_bomb = np.zeros(2, dtype=int)
    total_speed = np.zeros(5, dtype=int)
    total_actions = 0
    
    expert_dirs = [d for d in glob.glob(f"{data_dir}/expert_*") if os.path.isdir(d)]
    
    for expert_dir in expert_dirs:
        pkl_files = sorted(glob.glob(f"{expert_dir}/*.pkl"))
        
        for pkl_file in pkl_files:
            with open(pkl_file, 'rb') as f:
                data = pickle.load(f)
            
            for episode in data['episodes']:
                actions = np.array(episode['actions'])
                total_actions += len(actions)
                
                total_direction += np.bincount(actions[:, 0].astype(int), minlength=5)
                total_bomb += np.bincount(actions[:, 1].astype(int), minlength=2)
                total_speed += np.bincount(actions[:, 2].astype(int), minlength=5)
    
    print(f"æ€»åŠ¨ä½œæ•°: {total_actions:,}\n")
    
    # æ˜¾ç¤ºåˆ†å¸ƒ
    direction_labels = ['åœ', 'ä¸Š', 'ä¸‹', 'å·¦', 'å³']
    print("ã€æ–¹å‘ã€‘")
    for i, label in enumerate(direction_labels):
        pct = total_direction[i] / total_actions * 100
        print(f"  {label}: {total_direction[i]:7d} ({pct:5.2f}%)")
    
    bomb_labels = ['ä¸æ”¾', 'æ”¾ç‚¸å¼¹']
    print("\nã€ç‚¸å¼¹ã€‘â­ å…³é”®åŠ¨ä½œ")
    for i, label in enumerate(bomb_labels):
        pct = total_bomb[i] / total_actions * 100
        marker = " â† é‡è¦ï¼" if i == 1 else ""
        print(f"  {label}: {total_bomb[i]:7d} ({pct:5.2f}%){marker}")
    
    speed_labels = ['æœ€å¿«', 'ææ…¢', 'æ…¢', 'ä¸­', 'å¿«']
    print("\nã€é€Ÿåº¦ã€‘â­ å…³é”®åŠ¨ä½œ")
    for i, label in enumerate(speed_labels):
        pct = total_speed[i] / total_actions * 100
        marker = " â† é‡è¦ï¼" if 1 <= i <= 4 else ""
        print(f"  {label}: {total_speed[i]:7d} ({pct:5.2f}%){marker}")
    
    # è®¡ç®—é‡è¦æ€§æƒé‡
    direction_weights = np.array([1.0, 1.0, 1.0, 1.0, 1.0])  # æ–¹å‘éƒ½ä¸€æ ·
    bomb_weights = np.array([1.0, 20.0])  # æ”¾ç‚¸å¼¹æƒé‡ x20
    speed_weights = np.array([1.0, 5.0, 8.0, 8.0, 6.0])  # å‡é€Ÿæƒé‡ x5-8
    
    print(f"\n{'='*60}")
    print("æƒé‡ç­–ç•¥: importance (é‡è¦æ€§ä¼˜å…ˆ)")
    print(f"{'='*60}")
    print(f"æ–¹å‘æƒé‡: {direction_weights}")
    print(f"ç‚¸å¼¹æƒé‡: {bomb_weights}  â† æ”¾ç‚¸å¼¹æƒé‡x20")
    print(f"é€Ÿåº¦æƒé‡: {speed_weights}  â† å‡é€Ÿæƒé‡x5-8")
    print(f"{'='*60}\n")
    
    return {
        'direction': direction_weights,
        'bomb': bomb_weights,
        'speed': speed_weights,
        'distribution': {
            'direction': total_direction,
            'bomb': total_bomb,
            'speed': total_speed
        }
    }


def load_expert_trajectories(data_dir: str) -> List[types.Trajectory]:
    """åŠ è½½ä¸“å®¶æ•°æ®å¹¶è½¬æ¢ä¸º Trajectory æ ¼å¼"""
    print(f"{'='*60}")
    print(f"åŠ è½½ä¸“å®¶æ•°æ®ä»: {data_dir}")
    print(f"{'='*60}\n")
    
    trajectories = []
    expert_dirs = [d for d in glob.glob(f"{data_dir}/expert_*") if os.path.isdir(d)]
    
    if len(expert_dirs) == 0:
        raise ValueError(f"åœ¨ {data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸“å®¶æ•°æ®ç›®å½•")
    
    print(f"æ‰¾åˆ° {len(expert_dirs)} ä¸ªä¸“å®¶:")
    for expert_dir in expert_dirs:
        print(f"  - {os.path.basename(expert_dir)}")
    
    total_transitions = 0
    
    for expert_dir in expert_dirs:
        print(f"\nåŠ è½½ {os.path.basename(expert_dir)} çš„æ•°æ®...")
        pkl_files = sorted(glob.glob(f"{expert_dir}/*.pkl"))
        
        for pkl_file in pkl_files:
            with open(pkl_file, 'rb') as f:
                data = pickle.load(f)
            
            for episode in data['episodes']:
                obs_list = episode['observations']
                actions_list = episode['actions']
                next_obs_list = episode['next_observations']
                
                if len(obs_list) == 0:
                    continue
                
                # æ„å»ºå®Œæ•´è§‚å¯Ÿåºåˆ—
                full_obs_list = obs_list + [next_obs_list[-1]]
                
                # Flatten observations
                obs_arrays = []
                for obs in full_obs_list:
                    flattened_grid = obs['grid_view'].flatten()
                    flattened_obs = np.concatenate([flattened_grid, obs['player_state']])
                    obs_arrays.append(flattened_obs)
                
                trajectory = types.Trajectory(
                    obs=np.array(obs_arrays),
                    acts=np.array(actions_list),
                    infos=None,
                    terminal=True
                )
                
                trajectories.append(trajectory)
                total_transitions += len(obs_list)
        
        print(f"  âœ… åŠ è½½äº† {len(pkl_files)} ä¸ªæ–‡ä»¶")
    
    print(f"\n{'='*60}")
    print(f"âœ… æ€»å…±åŠ è½½:")
    print(f"   Episodes: {len(trajectories)}")
    print(f"   Transitions: {total_transitions:,}")
    print(f"{'='*60}\n")
    
    return trajectories


class WeightedBCTrainer(bc.BC):
    """
    å¸¦åŠ æƒæŸå¤±çš„BCè®­ç»ƒå™¨
    é‡å†™æŸå¤±è®¡ç®—ä»¥æ”¯æŒåŠ¨ä½œæƒé‡
    """
    def __init__(self, *args, action_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.action_weights = action_weights
        
        if action_weights is not None:
            # è½¬æ¢ä¸º tensor
            self.direction_weights = torch.tensor(
                action_weights['direction'], dtype=torch.float32
            )
            self.bomb_weights = torch.tensor(
                action_weights['bomb'], dtype=torch.float32
            )
            self.speed_weights = torch.tensor(
                action_weights['speed'], dtype=torch.float32
            )
    
    def _calculate_loss(self, obs, acts, **kwargs):
        """é‡å†™æŸå¤±è®¡ç®—ï¼ŒåŠ å…¥åŠ¨ä½œæƒé‡"""
        # è·å– policy çš„ distribution
        obs_tensor = torch.as_tensor(obs, device=self.device)
        acts_tensor = torch.as_tensor(acts, device=self.device)
        
        # å‰å‘ä¼ æ’­
        distribution = self.policy.get_distribution(obs_tensor)
        log_prob = distribution.log_prob(acts_tensor)
        
        # åŸºç¡€æŸå¤±
        neglogp = -log_prob
        
        # å¦‚æœæœ‰æƒé‡ï¼Œè®¡ç®—åŠ æƒæŸå¤±
        if self.action_weights is not None:
            # acts shape: (batch_size, 3)
            direction_acts = acts_tensor[:, 0].long()
            bomb_acts = acts_tensor[:, 1].long()
            speed_acts = acts_tensor[:, 2].long()
            
            # ç§»åŠ¨æƒé‡åˆ°è®¾å¤‡
            direction_w = self.direction_weights.to(self.device)
            bomb_w = self.bomb_weights.to(self.device)
            speed_w = self.speed_weights.to(self.device)
            
            # ä¸ºæ¯ä¸ªæ ·æœ¬è®¡ç®—æƒé‡
            sample_direction_w = direction_w[direction_acts]
            sample_bomb_w = bomb_w[bomb_acts]
            sample_speed_w = speed_w[speed_acts]
            
            # ç»„åˆæƒé‡ï¼ˆç‚¸å¼¹æƒé‡x2ï¼Œé€Ÿåº¦æƒé‡x1.5ï¼‰
            sample_weights = (sample_direction_w + sample_bomb_w * 2.0 + sample_speed_w * 1.5) / 4.5
            
            # åŠ æƒè´Ÿå¯¹æ•°ä¼¼ç„¶
            weighted_neglogp = neglogp * sample_weights
            neglogp_mean = weighted_neglogp.mean()
        else:
            neglogp_mean = neglogp.mean()
        
        # ç†µæ­£åˆ™åŒ–
        entropy = distribution.entropy().mean() if hasattr(distribution, 'entropy') else 0.0
        ent_loss = -self.ent_weight * entropy
        
        # L2 æ­£åˆ™åŒ–
        l2_norm = sum(p.pow(2).sum() for p in self.policy.parameters())
        l2_loss = self.l2_weight * l2_norm
        
        # æ€»æŸå¤±
        loss = neglogp_mean + ent_loss + l2_loss
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆç”¨äºç›‘æ§ï¼‰
        with torch.no_grad():
            predicted_acts = distribution.mode()
            if len(predicted_acts.shape) == 1:
                predicted_acts = predicted_acts.unsqueeze(-1)
            
            # MultiDiscrete: ä¸‰ä¸ªéƒ½å¯¹æ‰ç®—å¯¹
            all_correct = (predicted_acts == acts_tensor).all(dim=1).float().mean()
            
            # å„å­åŠ¨ä½œå‡†ç¡®ç‡
            direction_acc = (predicted_acts[:, 0] == acts_tensor[:, 0]).float().mean()
            bomb_acc = (predicted_acts[:, 1] == acts_tensor[:, 1]).float().mean()
            speed_acc = (predicted_acts[:, 2] == acts_tensor[:, 2]).float().mean()
        
        return {
            'loss': loss,
            'neglogp': neglogp_mean.item(),
            'entropy': entropy.item() if isinstance(entropy, torch.Tensor) else entropy,
            'ent_loss': ent_loss.item() if isinstance(ent_loss, torch.Tensor) else 0.0,
            'l2_loss': l2_loss.item(),
            'l2_norm': l2_norm.item(),
            'prob_true_act': all_correct.item(),
            'direction_acc': direction_acc.item(),
            'bomb_acc': bomb_acc.item(),
            'speed_acc': speed_acc.item(),
        }


def train_bc_weighted(
    data_dir: str = "./expert_data",
    output_dir: str = "./bc_models_weighted",
    n_epochs: int = 150,
    batch_size: int = 128,
    learning_rate: float = 3e-4,
    use_weights: bool = True,
):
    """ä½¿ç”¨åŠ æƒæŸå¤±è®­ç»ƒBCæ¨¡å‹"""
    print("="*60)
    print("BCè®­ç»ƒ - åŠ æƒæŸå¤±ç‰ˆæœ¬")
    print("="*60)
    print(f"\né…ç½®:")
    print(f"  æ•°æ®ç›®å½•: {data_dir}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  Epochs: {n_epochs}")
    print(f"  Batch Size: {batch_size}")
    print(f"  Learning Rate: {learning_rate}")
    print(f"  ä½¿ç”¨åŠ æƒ: {use_weights}")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"  è®¾å¤‡: {device}\n")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. åˆ†æåŠ¨ä½œåˆ†å¸ƒï¼ˆå¦‚æœä½¿ç”¨æƒé‡ï¼‰
    action_weights = None
    if use_weights:
        action_stats = analyze_action_distribution(data_dir)
        action_weights = {
            'direction': action_stats['direction'],
            'bomb': action_stats['bomb'],
            'speed': action_stats['speed']
        }
    
    # 2. åŠ è½½æ•°æ®
    trajectories = load_expert_trajectories(data_dir)
    
    # 3. åˆ›å»ºç¯å¢ƒ
    from environment import BomberEnv
    from gymnasium.wrappers import FlattenObservation
    
    base_env = BomberEnv()
    env = FlattenObservation(base_env)
    
    print(f"ç¯å¢ƒä¿¡æ¯:")
    print(f"  Observation Space: {env.observation_space}")
    print(f"  Action Space: {env.action_space}\n")
    
    # 4. åˆ›å»º Policy
    print(f"{'='*60}")
    print("åˆ›å»ºPolicyç½‘ç»œ...")
    print(f"{'='*60}\n")
    
    rng = np.random.default_rng(seed=42)
    
    policy_kwargs = dict(
        net_arch=dict(pi=[512, 256, 128], vf=[512, 256, 128])  # å¤§ç½‘ç»œï¼Œæå‡å®¹é‡
    )
    
    policy = ActorCriticPolicy(
        observation_space=env.observation_space,
        action_space=env.action_space,
        lr_schedule=lambda _: learning_rate,
        **policy_kwargs
    ).to(device)
    
    print(f"âœ… Policyå·²åˆ›å»º (net_arch=[512, 256, 128])")
    
    # 5. åˆ›å»ºBCè®­ç»ƒå™¨ï¼ˆä½¿ç”¨åŠ æƒç‰ˆæœ¬ï¼‰
    if use_weights:
        bc_trainer = WeightedBCTrainer(
            observation_space=env.observation_space,
            action_space=env.action_space,
            demonstrations=trajectories,
            policy=policy,
            rng=rng,
            batch_size=batch_size,
            ent_weight=1e-3,
            l2_weight=5e-5,
            device=device,
            action_weights=action_weights,
        )
        print(f"âœ… åŠ æƒBCè®­ç»ƒå™¨å·²åˆ›å»º")
        print(f"   ç‚¸å¼¹æŸå¤±æƒé‡æå‡ x2.0")
        print(f"   é€Ÿåº¦æŸå¤±æƒé‡æå‡ x1.5\n")
    else:
        bc_trainer = bc.BC(
            observation_space=env.observation_space,
            action_space=env.action_space,
            demonstrations=trajectories,
            policy=policy,
            rng=rng,
            batch_size=batch_size,
            ent_weight=1e-3,
            l2_weight=1e-4,
            device=device,
        )
        print(f"âœ… æ ‡å‡†BCè®­ç»ƒå™¨å·²åˆ›å»º\n")
    
    # 6. è®­ç»ƒ
    print(f"{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒ {n_epochs} epochs...")
    print(f"{'='*60}\n")
    
    try:
        bc_trainer.train(n_epochs=n_epochs)
        
        print(f"\n{'='*60}")
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"{'='*60}\n")
        
    except KeyboardInterrupt:
        print(f"\n{'='*60}")
        print("âš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"{'='*60}\n")
    
    # 7. ä¿å­˜æ¨¡å‹
    policy_path = f"{output_dir}/bc_policy_weighted.pth"
    torch.save(bc_trainer.policy.state_dict(), policy_path)
    print(f"âœ… Policyå·²ä¿å­˜: {policy_path}")
    
    policy_obj_path = f"{output_dir}/bc_policy_weighted.pt"
    torch.save(bc_trainer.policy, policy_obj_path)
    print(f"âœ… Policyå¯¹è±¡å·²ä¿å­˜: {policy_obj_path}")
    
    # 8. è½¬æ¢ä¸ºPPOæ ¼å¼
    print(f"\n{'='*60}")
    print("è½¬æ¢ä¸ºPPOæ ¼å¼...")
    print(f"{'='*60}\n")
    
    try:
        # ä½¿ç”¨ä¸BCç›¸åŒçš„ç½‘ç»œç»“æ„
        ppo_policy_kwargs = dict(
            net_arch=dict(pi=[512, 256, 128], vf=[512, 256, 128])
        )
        
        ppo_model = PPO(
            policy=ActorCriticPolicy,
            env=env,
            policy_kwargs=ppo_policy_kwargs,  # æŒ‡å®šç½‘ç»œç»“æ„
            learning_rate=learning_rate,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
            verbose=1,
            device=device,
        )
        
        ppo_model.policy.load_state_dict(bc_trainer.policy.state_dict(), strict=False)
        
        ppo_path = f"{output_dir}/bc_ppo_weighted.zip"
        ppo_model.save(ppo_path)
        print(f"âœ… PPOæ ¼å¼å·²ä¿å­˜: {ppo_path}")
        print(f"   100%å‚æ•°è½¬ç§»å®Œæˆï¼ˆç½‘ç»œç»“æ„ä¸€è‡´ï¼‰")
        
    except Exception as e:
        print(f"âš ï¸  PPOè½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # 9. æ€»ç»“
    print(f"\n{'='*60}")
    print("è®­ç»ƒå®Œæˆæ€»ç»“")
    print(f"{'='*60}")
    print(f"âœ… Policy: {policy_obj_path}")
    if 'ppo_path' in locals():
        print(f"âœ… PPOæ ¼å¼: {ppo_path}")
        print(f"\nğŸ® ä½¿ç”¨æ–¹æ³•:")
        print(f"   robot.py ä¼šè‡ªåŠ¨åŠ è½½æ­¤æ¨¡å‹")
    
    if use_weights:
        print(f"\nğŸ¯ åŠ æƒè®­ç»ƒä¼˜åŠ¿:")
        print(f"   âœ“ æ”¾ç‚¸å¼¹åŠ¨ä½œå­¦ä¹ æƒé‡æå‡")
        print(f"   âœ“ å‡é€Ÿèµ°ä½å­¦ä¹ æƒé‡æå‡")
        print(f"   âœ“ é¢„æœŸæ¸¸æˆè¡¨ç°: ä¼šä¸»åŠ¨æ”¾ç‚¸å¼¹å’Œç²¾ç¡®èµ°ä½")
    
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"   1. å¯åŠ¨ robot.py æµ‹è¯•æ¸¸æˆè¡¨ç°")
    print(f"   2. è§‚å¯Ÿæ˜¯å¦ä¼šåœ¨åˆé€‚æ—¶æœºæ”¾ç‚¸å¼¹")
    print(f"   3. è§‚å¯Ÿæ˜¯å¦ä¼šå‡é€Ÿèº²é¿ç‚¸å¼¹")
    print(f"   4. å¦‚æœæ•ˆæœå¥½ï¼Œè¿›å…¥PPOå¾®è°ƒé˜¶æ®µ")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="BCè®­ç»ƒï¼ˆåŠ æƒæŸå¤±ç‰ˆï¼‰")
    parser.add_argument("--data_dir", type=str, default="./expert_data")
    parser.add_argument("--output_dir", type=str, default="./bc_models_weighted")
    parser.add_argument("--n_epochs", type=int, default=150)
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--no_weights", action="store_true", help="ç¦ç”¨åŠ æƒæŸå¤±")
    
    args = parser.parse_args()
    
    train_bc_weighted(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        n_epochs=args.n_epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        use_weights=not args.no_weights,
    )

